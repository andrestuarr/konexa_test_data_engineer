# Konexa Test - Data Engineer

Este repositorio presenta una soluciÃ³n integral para una prueba tÃ©cnica de **IngenierÃ­a de Datos**, basada en una **arquitectura orientada a eventos** en Google Cloud Platform. La soluciÃ³n aborda tres requerimientos clave mediante el uso de Terraform, Cloud Functions, Composer (Airflow) y BigQuery.

---

## ğŸ¯ Objetivo

1. Detectar automÃ¡ticamente la carga de un archivo en un bucket de GCS.
2. Ejecutar una Cloud Function que reaccione al evento.
3. Lanzar un DAG en Cloud Composer que importe el archivo en BigQuery y realice una transformaciÃ³n de datos.

---

## âš™ï¸ Arquitectura de la soluciÃ³n

Se implementa una **arquitectura event-driven** (basada en eventos) que automatiza completamente el procesamiento del archivo:

<img src="imagenes/arquitectura.jpg" alt="Arquitectura de la soluciÃ³n" width="750"/>

### Flujo:

1. âœ… **Carga de archivo a GCS**  
   Un archivo `.csv` es subido manual o automÃ¡ticamente a un bucket de Cloud Storage, lo que genera un evento.

2. ğŸš€ **Cloud Function (trigger por evento)**  
   La funciÃ³n se activa por el evento, extrae los metadatos del archivo y lanza un DAG en Composer, pasando los parÃ¡metros necesarios.

3. ğŸ“ˆ **DAG en Cloud Composer (Airflow)**  
   El DAG realiza las siguientes acciones:
   - Verifica o crea el dataset en BigQuery.
   - Carga el archivo desde GCS a una tabla temporal en BigQuery.
   - Aplica una transformaciÃ³n SQL para tipificar correctamente los datos y almacenarlos en una tabla final.

<img src="imagenes/dag.png" alt="Diagrama del DAG ejecutado exitosamente" width="750"/>

---

## ğŸ“ Estructura del proyecto

```plaintext
â”œâ”€â”€ cloud_functions/            # CÃ³digo de la Cloud Function
â”‚   â””â”€â”€ main.py
|   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ composer/                   # DAG de Airflow
â”‚   â””â”€â”€ dag_process.py
â”‚
â”œâ”€â”€ infrastructure/             # Infraestructura como cÃ³digo (Terraform)
â”‚   â”œâ”€â”€ modules
â”‚   â”‚    â”œâ”€â”€ cloud_functions
â”‚   â”‚    â”œâ”€â”€ cloud_storage
â”‚   â”‚    â”œâ”€â”€ composer
â”‚   â”‚    â””â”€â”€ iam
|   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ providers.tf
â”‚   â””â”€â”€ outputs.tf
â”‚
â””â”€â”€ imagenes/                   # Diagramas explicativos
    â”œâ”€â”€ arquitectura.jpg
    â””â”€â”€ dag.png
```


---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Google Cloud Platform (GCP)**  
  - Cloud Storage  
  - Cloud Functions  
  - Cloud Composer (Apache Airflow)  
  - BigQuery  

- **Terraform** â€” Infraestructura como cÃ³digo  
- **GitHub Actions** â€” AutomatizaciÃ³n del CI/CD  
- **Python** â€” LÃ³gica de la Cloud Function y el DAG

---

## ğŸš€ Despliegue en otro proyecto

1. **Clonar este repositorio** y configurar las credenciales de GCP.  
   Debes contar con una cuenta de servicio con permisos para desplegar recursos (Storage, Functions, Composer, etc.). Guarda el JSON como secreto en GitHub con el nombre `GCP_CREDS`.

2. **Configurar variables de entorno**  
   Edita las variables `PROJECT_ID` y `REGION` en los secretos o variables del pipeline. AdemÃ¡s, revisa `variables.tf` para evitar conflictos con nombres de buckets o recursos.

3. **Desplegar con Terraform**  
   AsegÃºrate de que las siguientes APIs estÃ©n habilitadas: Cloud Storage, Eventarc, Cloud Functions, Cloud Composer, BigQuery.

   ```bash
   cd infrastructure
   terraform init
   terraform apply
4. **Probar despliegue automÃ¡tico**  
   Realiza un cambio en alguno de los siguientes componentes para activar el pipeline CI/CD:
   - CÃ³digo del DAG (`composer/dags`)
   - CÃ³digo de la Cloud Function (`modules/cloud_function`)
   - Infraestructura (`infrastructure`)

   âš ï¸ Cambios en `modules/cloud_storage/function.zip` no activarÃ¡n el pipeline, ya que estÃ¡n ignorados por configuraciÃ³n.

   ```bash
   git add .
   git commit -m "Prueba de despliegue automÃ¡tico"
   git push origin main
<img src="imagenes/despliegue.png" alt="DescripciÃ³n" width="900">

5. **Probar el flujo de procesamiento de datos**  
   Sube el archivo `Telecom_Customers_Churn.csv` al bucket creado por Terraform. Esto inicia automÃ¡ticamente el pipeline de procesamiento:

   - âœ… La **Cloud Function** se activa al detectar la subida del archivo.
   - ğŸš€ Lanza un **DAG en Cloud Composer**, pasÃ¡ndole los metadatos del evento.
   - ğŸ“ˆ El **DAG ejecuta las siguientes tareas**:
     - Verifica si el **dataset** en BigQuery existe; si no, lo crea.
     - Carga el archivo CSV desde GCS a una tabla **temporal** en BigQuery, con todos los campos como `STRING`.
     - Aplica una transformaciÃ³n SQL que **castea las columnas a sus tipos correctos** usando un esquema `.json` predefinido almacenado en GCS (tambiÃ©n creado con la IaC).

---