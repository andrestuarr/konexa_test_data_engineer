# Konexa Test - Data Engineer

Este proyecto es una soluciÃ³n integral para una prueba tÃ©cnica de ingenierÃ­a de datos. La soluciÃ³n aborda tres requerimientos principales mediante una arquitectura basada en eventos:

## ğŸ§© Objetivo

1. Procesar automÃ¡ticamente un archivo cuando se carga a un bucket en GCS.
2. Desplegar automÃ¡ticamente una Cloud Function para manejar el procesamiento.
3. Ejecutar un DAG en Cloud Composer para cargar el archivo en BigQuery y aplicar una transformaciÃ³n.

---

## âš™ï¸ Arquitectura de la soluciÃ³n

La soluciÃ³n implementa una **arquitectura orientada a eventos** que cubre las tres preguntas:

<img src="imagenes/arquitectura.jpg" alt="DescripciÃ³n" width="750">


1. âœ… **Subida de archivo a GCS**  
   Dispara un evento que activa una Cloud Function.

2. ğŸš€ **Cloud Function (Event Trigger)**  
   La funciÃ³n obtiene los metadatos del archivo y lanza un DAG en Cloud Composer con los datos del evento.

3. ğŸ“ˆ **DAG en Composer (Airflow)**  
   El DAG:
   - Asegura la existencia del dataset en BigQuery.
   - Carga el archivo desde GCS a BigQuery.
   - Ejecuta una transformaciÃ³n SQL directamente sobre los datos cargados.



   <img src="imagenes/dag.png" alt="DescripciÃ³n" width="750">

---

## ğŸ“ Estructura del proyecto



---

## ğŸ› ï¸ TecnologÃ­as usadas

- **Google Cloud Platform (GCP)**  
  - Cloud Functions  
  - Cloud Storage  
  - BigQuery  
  - Cloud Composer (Airflow)

- **Terraform** â€” Para Infraestructura como CÃ³digo  
- **GitHub Actions** â€” Para CI/CD automatizado  
- **Python** â€” LÃ³gica de Cloud Function y DAG

---

## ğŸš€ Despliegue en otros proyectos

1. Clonar el repositorio y configurar credenciales GCP (Es necesario contar con una cuenta de servicio con los permisos necesarios para desplegar los servicios y guardarla como secreto con el nombre GCP_CREDS).
2. Editar las variables de entorno del pipeline PROJECT_ID y REGION. AdemÃ¡s teber en cuenta las variables en variables.tf de la raiz infrasctructure para evitar conflicto con el nombre de los buckets.
3. Ejecutar los comandos de Terraform (Tener en cuenta que se deben tener habilitadas las API de GCS, Eventract, Cloud Functios, Composer):
   ```bash
   cd infrastructure
   terraform init
   terraform apply
4. Para probar el despligue atomatico, se tendrÃ­a que hacer un cambio en el DAG, en el cÃ³digo main de funciÃ³n o algun script dentro de infrastructure. 
    ```bash
   git add .
   git commit -m"Prueba de despligue automatizado"
   git push origin main

<img src="imagenes/despliegue.png" alt="DescripciÃ³n" width="900">

5. Para probar el flujo de los datos, se tendrÃ­a que subir un archivo al bucket source y esperar a que se termine de ejecutar el dag para visualizar los datos transformados en bigquery.